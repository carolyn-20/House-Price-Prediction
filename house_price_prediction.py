# -*- coding: utf-8 -*-
"""House price prediction

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13LpILga9yJN41dyszglLlLdeFfNkWoFb
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('housing.csv')

df.dropna(inplace = True)

df.info()

df.dropna(inplace=True)

from sklearn.model_selection import train_test_split

X = df.drop(['median_house_value'], axis = 1)
y = df['median_house_value']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)

train_data = X_train.join(y_train)

train_data.hist(figsize=[50,25])

#from sklearn.preprocessing import LabelEncoder

#train_data['ocean_proximity'] = LabelEncoder().fit_transform(train_data['ocean_proximity'])
train_data = train_data.join(pd.get_dummies(train_data['ocean_proximity'])).drop(['ocean_proximity'], axis = 1)
plt.figure(figsize = (10,10))
sns.heatmap(train_data.corr(), annot = True, cmap = 'YlGnBu')

train_data['total_rooms'] = np.log(train_data['total_rooms'] + 1)        #log is used to compress the values into small numbers to calculate easily.
train_data['total_bedrooms'] = np.log(train_data['total_bedrooms'] + 1)
train_data['population'] = np.log(train_data['population'] + 1)
train_data['households'] = np.log(train_data['households'] + 1)

train_data.hist(figsize=(20,15))

plt.figure(figsize = (10,10))
sns.heatmap(train_data.corr(), annot = True, cmap = 'YlGnBu')

plt.figure(figsize=(10,5))
sns.scatterplot(x = 'latitude', y = 'longitude', data = train_data, hue = 'median_house_value', palette = 'coolwarm')

train_data['bedroom_ratio'] = train_data['total_bedrooms'] / train_data['total_rooms']
train_data['household_rooms'] = train_data['total_rooms'] / train_data['households']

plt.figure(figsize = (10,10))
sns.heatmap(train_data.corr(), annot = True, cmap = 'YlGnBu')

from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler() #Mathematical operations

# Now redefine X_train and y_train
X_train, y_train = train_data.drop(['median_house_value'], axis=1), train_data['median_house_value']
X_train_s  = scaler.fit_transform(X_train)

# Fit the model
reg = LinearRegression()
reg.fit(X_train_s, y_train)

test_data = X_test.join(y_test)

test_data['total_rooms'] = np.log(test_data['total_rooms'] + 1)          # Log of 1 becomes log(0 + 1) = log(1) = 0 because log(0) = 1.
test_data['total_bedrooms'] = np.log(test_data['total_bedrooms'] + 1)
test_data['population'] = np.log(test_data['population'] + 1)
test_data['households'] = np.log(test_data['households'] + 1)

test_data['bedroom_ratio'] = test_data['total_bedrooms'] / test_data['total_rooms']
test_data['household_rooms'] = test_data['total_rooms'] / test_data['households']

X_test, y_test = test_data.drop(['median_house_value'], axis = 1), test_data['median_house_value']

X_test = X_test.reindex(columns=X_train.columns, fill_value=0) #It is used to rearrange the columns of X_test to match the order of the columns in X_train. This ensures that both datasets have the same column order.

X_test_s = scaler.transform(X_test)

reg.score(X_test_s, y_test)

from sklearn.ensemble import RandomForestRegressor

forest = RandomForestRegressor()
forest.fit(X_train_s, y_train)

forest.score(X_test_s, y_test)

from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [3, 10, 30], #Values are tress to explore smaller to larger values.
    'max_features': [8, 12, 20],
    'min_samples_split': [2, 4, 6,8]
}
grid_search = GridSearchCV(forest, param_grid, cv=5, scoring='neg_mean_squared_error', return_train_score=True)
grid_search.fit(X_train_s, y_train)

grid_search.best_estimator_
grid_search.best_estimator_.score(X_test_s, y_test)